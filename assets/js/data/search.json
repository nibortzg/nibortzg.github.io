[ { "title": "(論文筆記) Designing Network Design Spaces -&gt; Regnet", "url": "/posts/Designing-Network-Design-Space/", "categories": "Computer Vision, Design Paragigm", "tags": "Computer Vision, CVPR, FAIR, Design Paragigm", "date": "2021-01-23 12:30:00 -0800", "snippet": "前言:這篇論文試圖使用統計的方法來縮減design space，一步步的將AnyNet 縮減成RegNet, 並結總結出幾個有用的設計準則。雖然模型只在imagenet 上 train 10 epoch 讓公信力有點不足，不仍然值得嘗試。方法:首先作者先定義了一種AnyNet, 而這個Anynet由以下幾種參數定義，每個stage的block 數量 $d_i$ ,每個stage寬度(channels) $w_i$ , bottleneck ratio $b_i$, group width $g_i$ (如果 $g_i == 1$ 則為depthwise convolution)如何將AnyNet的搜索空間縮減過程就不深究，有興趣直接看一下論文，基本上都是作者對於EDF(Empirical distrubution function)的比較，然後選定優化方向。在縮減過程中作者得到幾個結論 $AnyNetX_B$ &amp;amp; $AnyNetX_C$ 若所有的 $b_i = b$ 且 $g_i = g$, 所得到EDF的並不會變化,所以accuracy 並不會有所損失 $AnyNetX_D$ &amp;amp; $AnyNet_E$ 若 $w_{i+1} \\geq w_i$ 且 $d_{i+1} \\geq d_{i}$ 則可以讓整體精確度上升之後作者在AnyNet的Design space 中找出表現好得model，並且使用quentized linear parameterization (Linear fit) 試圖去總結出一些結論，然後他發現可以fit這條線越好的model有越好的performance.並給出了RegNet fit 出來的結論 最好的model 大約 20 stages (60 layers) bottleneck ration = 1 是最好的, 移除 bottleneck !! Width mutltiplier 為 2.5 最佳, 跟常用的2 差不多複雜度分析:activation 定義: the sizeof the output tensor of all conv layersRun time 與 activation 的關係比flops 更加緊密圖為最好的12個模型，可以看出在低parameters 時, 每個stage的 blocks 數目會逐漸上升，但是在大一些的模型，在stage 3 的blocks 數目會很多，但是stage 4 的數目會很少Ablation study 結論: inverted bottleneck 會使performance 下降 Swish vs ReLU: swish 在低flops 時比較好，ReLU 在高flops 比較好, 且swish 在depthwise conv 時表現得比ReLU 好很多 SE (squeeze and excitation) 會增加精度" }, { "title": "(論文筆記) A ConvNet for the 2020s", "url": "/posts/A-ConvNet-for-the-2020s/", "categories": "Computer Vision, Design Paragigm", "tags": "Computer Vision, FAIR, Design Paragigm", "date": "2021-01-20 02:33:00 -0800", "snippet": "前言:近幾年vision transformer 在各種會議上大放異彩, 隨著演進, 我們也可以發現transformer 引進越來越多跟CNN 相關的prior, 像是swin transformer的local window 就很有ConvNet的味道。觀察中可以發現vision transformer 和 CNN 變得越來越像, 且在一些下游任務中, 這些內建於CNN之中的bias會使得任務更好完成, 例如translation equivariance 對於object detection任務而言就很重要。第二， 對於現在vision transformer 成功的關鍵，是在於transformer 本身，還是training techniques與一些其餘組件的改動，造成整體performance 的提升也是值得思考得問題，例如專精於training techniques 提昇讓Resnet 復活的論文ResNet strikes back: An improved training procedure in timm 或是 Patches are all you need 討論patches是否才是成功關鍵，都是值得思考的問題。作者在此篇論文像是抄作業一般將他認為swin transformer 勝過於目前Resnet的關鍵複製過去，一步一步的 “modernizing” Resnet.方法:1. 訓練技術改進(Training techniques)在前面提到timm 那篇論文中有提到光靠訓練技術(data augumentation, optimizer, training receipts)的進步就可以將Resnet50 上調到80.4, 但作者在這邊為了方便與swin transformer 比較, 使用與他們相近的訓練技術。光是這樣performance 就從76.1% -&amp;gt; 78.8% 有了2.7% 的進步。2. 改變stage 的compute ratio將原本Resnet 每個stage 的block 排列(3,4,6,3) 改成跟 Swin-T 一樣比例的 (3,3,9,3)也提升了performance。 值得注意的是，block 數目也從16 -&amp;gt; 18。 感覺performance 提升在這裡是跟多運算比較有關係。不過運算的前後分配也是一個值得思考的點。作者也給了兩篇參考文獻: On network design spaces for visual recognition 和 Designing network design spaces.3. 將stem 改成patch 形式Stem 就是圖片進到CNN的第一層結構, 由於一般圖片都有很多冗餘資訊，所以基本上會採用很激進的dwonsample 策略，將input image縮成適當比例的feature map 再做操作。原本的Resnet 是使用7x7 conv with stride 2與max pooling 連結達到4倍 downsampling 的結果, 這裡使用激進的patchify做法，也就是使用large kernel size with non-overlapping convolution. 在此作者使用 4x4 convolution with stride 4. stem = nn.Sequential( nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4), LayerNorm(dims[0], eps=1e-6, data_format=&quot;channels_first&quot;) )4. Bottleneck 設計 -&amp;gt; ResNeXt-ifyResNext 的設計宗旨就是用更多的group 來換取更多的width。 在此作者使用MobileNet 所使用的 depthwise convolution (group=channel 的group convolution). 可以注意的是depthwise convolution 很像self-attention中的加權和，他們都是只把spatial dimension 的資訊混合而沒有讓資訊在channel中流通。5. Bottleneck 設計 -&amp;gt; inverted bottleneck跟原本Mobilenet V2 的inverted bottleneck (b)相比, 作者這邊先把 depthwise convolution 位置從中間調整到最上面，主要是為了減少計算量，再者他將kernel size 變大成7x7, 實驗結果表示在7x7 之後更大的kernel size 並沒有將performance 提升6. ReLU -&amp;gt; GELUGELU 是ReLU的平滑版本, 在很多transformer 中都有使用像是Bert, GPT-2作者在此使用並沒有讓準確度提升, 在實際應用上我應該也會傾向使用ReLU畢竟速度快又有比較多的library 支援7. 更少的activation function 與 normalization layer如圖所示, 跟一般操作不同的是, 不一定要在每個 convolution layer後面加上activation function and BN.8. 將BN換成LN (Layer Normalization)BN 在ConvNet中可以穩定training 也可以減少過擬合, 然而近期的研究指出BN也會讓performance 降低, 且LN 大量的被用在transformer中。所以作者決定將BN換成LN。 (注:直接將Resnet50 中的BN 換成LN會使得performance 下降)9. Downsampling layer 改成patch 形式downsample_layer = nn.Sequential( LayerNorm(dims[i], eps=1e-6, data_format=&quot;channels_first&quot;), nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2), )若直接將downsampling layer 改成non-overlapping 的convolution, CNN會發散，所以要加入 normalization layers 來穩定training重要結論: 以前認為Vision transformer 有比較少 inductive bias(prior) 所以在有大量pretrain data的情況下可以學得比較好， 但是ConvNext 實驗證明， 好好設計的ConvNet 並不會輸。" } ]
